---
title: "Fighting Twitterbots or why Elon Musk wont buy Twitter"
description: |
 Detecting and Evaluating twitterbots in a #Drosten relatet dataset!
output: 
postcards:: jolla
site: distill::distill_website
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating websites with Distill at:
# https://rstudio.github.io/distill/website.html

```


## Why is this important? 

Detecting non-human Twitter users has been of interest not only to Elon Musk but also to academics. Indiana University has developed a free service called **Botometer**, which scores Twitter handles based on their likelihood of being a Twitterbot. Already in 2009, Twitter bots were estimated to create approximately 24% of tweets on Twitter. One academic study in 2017 estimated that **up to 15%** of Twitter users were automated bot accounts. And the prevalence of Twitter bots coupled with the ability of some bots to give seemingly human responses has enabled these non-human accounts to garner **widespread influence**, in both positive and negative ways. 

This project will address the question whether a **specific twitter dataset** - consisting of all tweets mentioning #Drosten or @c_drosten in the years 2020 and 2021 - contains bots and how they evolve over time. 

Furthermore, the project wants to find out whether there is a certain emotionality in the tweets of the bots and whether this differs from the human like tweets. 


## Research Questions

1. Which percentage of the total number of tweets originate from bots and does this change as the pandemic progresses?  

2. Are bots and non-bots taking talking in a different valence?

## The data


## Methods

**Botometer**

1. Step [ ] Process all tweets by using the botometer API: https://botometer.osome.iu.edu

2. Step [ ] Interpret the results and find a suited threshold 

3. Step [ ] Analyse the results: Frequency and distribution over time 

**Sentiment Analysis**

4. Step [ ] Perform Sentiment Analysis with the tweets indenified as "bot-like" vs "human-like"



## Implications

This small project attempts to contribute to the detection of possible bots in a Twitter dataset related to tweets mentioning Christian Drosten using #drosten or @c_drosten. It can provide us with information about the **extent to which these Twitter hashtags have been used by bots**. 

In addition, the value of bot tweets can tell us more about the origin and goals of the bots created and provide an answer to the question of **whether bots were pushing certain issues**.   
 



```{r, include=FALSE}
library(readr)
library(stringr)
library(tidyverse)
library(tidytext)
library("rjson")
library(plyr)
library(sentimentr)
library(lubridate)
library(jsonlite)
library(rcompanion)
library("miceadds")
library(patchwork)
library(readxl)
library(wordcloud)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
###
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r, warning=FALSE, message= FALSE}
# load sentiment files
negativ <- read.table("~/Documents/GitHub/Analysing-Drosten/1_data/SentiWS_v2.0_Negative.txt", fill = TRUE,encoding = "UTF-8")
positiv <- read.table("~/Documents/GitHub/Analysing-Drosten/1_data/SentiWS_v2.0_Positive.txt", fill = TRUE,encoding = "UTF-8")


#delete the NN stuff
negativ = separate(data = negativ, col = V1, sep =  "[|]", into = "V1")
negativ = negativ %>% filter(!V3=="")
positiv = separate(data = positiv, col = V1, sep =  "[|]", into = "V1")

#split into single words
einzelworte_negativ <- strsplit(as.character(negativ$V3), split =",")
einzelworteframe_negativ <- as.data.frame(unlist(einzelworte_negativ))

einzelworte_positiv <- strsplit(as.character(positiv$V3), split =",")
einzelworteframe_positiv <- as.data.frame(unlist(einzelworte_positiv))

#takes the number of words and creates a data frame only with the sentiment scores as many times as the word inflection occurs.
number_words <- summary(einzelworte_negativ)

sentiment_score <- NULL
for (i in 1:1759) {
  j <- 0
  while (j < as.numeric(number_words[i])) {
    sentiment_score <- rbind(sentiment_score, negativ[i,2])
    j <- j+1
  }
}

##
new_negativ <- cbind(as.character(einzelworteframe_negativ[,1]), sentiment_score)
new_negativ <- rbind(negativ[,1:2], new_negativ)

# same for positiv
number_words <- summary(einzelworte_positiv)

sentiment_score <- NULL
for (i in 1:1644) {
  j <- 0
  while (j < as.numeric(number_words[i])) {
    sentiment_score <- rbind(sentiment_score, positiv[i,2])
    j <- j+1
  }
}

#bind the sentiment score with the words
new_positiv <- cbind(as.character(einzelworteframe_positiv[,1]), sentiment_score)
new_positiv <- rbind(positiv[,1:2], new_positiv)


lexicon = bind_rows(new_positiv, new_negativ)
#lexicon = lexicon %>% select(V1, V2) %>% rename(polarity = V2, word = V1) %>% mutate_all(.funs=tolower)
lexicon$V2 = as.numeric(lexicon$V2)
lexicon$V1 = str_to_lower(lexicon$V1)
lexikey = as_key(lexicon)

####### tokenize tweets #####################


# lexikey 
lexikey$word <- lexikey$x
lexikey$value <- lexikey$y
lexikey <- lexikey %>% select(word,value)


```

```{r}
#### tweets einlesen ####
all_tweets_botchecked <- readRDS("~/Desktop/all_tweets_botchecked.RDS")
###

require(lubridate)

tweets <- all_tweets_botchecked %>% 
  as_tibble() %>% readr::type_convert() %>% 
  select(created_at, id, text, entities, in_reply_to_user_id, 
         author_id, cap.universal, lang, botcheck) %>% 
  dplyr::mutate(main = str_to_lower(text),
         id = 1:dplyr::n()) %>% 
  dplyr::rename(time = created_at) %>%
  mutate(year_index = lubridate::year(time)-2020,
         yday = lubridate::yday(time) + year_index*365,
         yweek = lubridate::week(time) + year_index*52)


tweets_sent <- tweets %>% 
  unnest_tokens(word, main) %>% 
  inner_join(lexikey) %>% 
  dplyr::group_by(id) %>% 
  dplyr::summarize(value = mean(value))

tweets = tweets %>% 
  left_join(tweets_sent, by = "id")

tweets_agg = tweets %>% 
  filter(!is.na(botcheck), !is.na(value)) %>%
  dplyr::group_by(yweek, botcheck) %>% 
  dplyr::summarize(sentiment = mean(value))


```

```{r , include=FALSE}
### tweets verarbeiten


```

#### wordcloud visualisation (so far)
```{r, warning=FALSE, message=FALSE}
stopwords <- stopwords("german")

tweets_word <- tweets %>% 
  unnest_tokens(word, main)

token_frequencies <- table(tweets_word$word)
token_frequencies <- token_frequencies %>% sort(decreasing = TRUE)
stops <- names(token_frequencies) %in% stopwords 

docs <- tweets_word %>% filter(!word %in% stopwords)

wordcloud(words = docs$word,  min.freq = 15,
          max.words=500, rot.per=0.35, #ordered.colors=TRUE,
          scale = c(3.5,1.0), random.order= FALSE, 
          colors=c("#67a9cf", "#bdc9e1", "#74a9cf", "#2b8cbe", "#045a8d"))
```
## Results 

### Threshold 


```{r}
hist(all_tweets_botchecked$cap.universal)
abline(v = 0.8, col = "red")
hist(all_tweets_botchecked$raw_scores.universal.overall)
```


### How many Bots are in the dataset? 

```{r, warning=FALSE, message=FALSE}
p <- tweets %>%
  select(yweek, botcheck) %>%
  drop_na() %>%
  ggplot(mapping = aes(x= yweek, fill=botcheck))+
  facet_wrap(~botcheck)

p + geom_histogram() + theme_minimal()+ 
  ggtitle("Total Tweets over Time in the #Drosten dataset - CAP 0.8")+
  xlab("Time in Weeks from 2020 - 2021")+ 
  ylab("frequency")
```

### Sentiment 

```{r, warning=FALSE, message= FALSE}
ggplot(tweets_agg, aes(x=yweek, y=sentiment, col = botcheck)) + 
  geom_line() + ylim(-.2, .2) + theme_minimal() +
  theme(legend.text = element_text(size=10),
        axis.text.x = element_text(size=10, angle=90), 
        axis.text.y = element_text(size=10))
```

## Limitations
```{r, warning=FALSE, message= FALSE}
